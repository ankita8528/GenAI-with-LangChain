# LangChain: Models
In LangChain, "Models" are the core interfaces through which you interact with AI models. 

## 1. Brief History
At some time in the past, everyone was curious to make a **chatbot**. There were 2 major problems:
- NLU (Natural Language Understanding), because how to make the system understand the human language & context.
- Correct Response (context aware text generation).

Then there came a time when LLMs were introduced and both the problems solved. LLMs trained on huge internet data and hence they have billions of parameters and they are of huge size (~100GB). So, no one can use them locally.

Again a problem, because to inference such a heavy models we need much powerful GPUs which are expensive. This problem was resolved with the introduction of **API**. Builders uploaded the LLMs on their server and created an API, so that anyone can use it.

But then there arises this 3rd problem of "how to implement?", because of the behaviour of different APIs were different. There comes our saviour **LangChain**, LangChain created an interface that helps using any API in a standarddized fashion.

In short, **Models** in **LangChain** standardizes the interface of interacting with any AI model.

## 2. Two Main Types of Models in LangChain
LangChain categorizes models into two types:

+ Language Models → Takes text in, gives text out. Example: GPT-3, GPT-4
+ Embedding Models → These models convert text into numbers (vectors). Example: text-embedding-ada-002

Under the Language Models, there are 2 subcategories:
+ LLM Models
+ Chat Models

  <img width="788" height="445" alt="image" src="https://github.com/user-attachments/assets/171b697b-99c4-48d4-815a-c9c92df29dd3" />


## 3. Let’s Code – First Step with LLMs

We’ll start with OpenAI (since it’s most popular).

```
from langchain_openai import OpenAI
# Create a simple model instance
llm = OpenAI(model="text-davinci-003", temperature=0.7)
# Ask the model something
response = llm.invoke("What is Artificial Intelligence in one line?")
print(response)
```

> Output:
>
> Artificial Intelligence is the simulation of human intelligence by machines.

## 4. Chat Models – More Natural Conversations

Here’s how we use Chat Models:

```
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
chat_model = ChatOpenAI(model="gpt-3.5-turbo")
messages = [
    SystemMessage(content="You are a helpful teacher."),
    HumanMessage(content="Explain AI like I’m 5 years old.")
  ]
response = chat_model.invoke(messages)
print(response.content)
```

> Output:
> 
> AI is like teaching a toy robot to think and talk like a human.

## 5. Why Does LangChain Wrap Models?

Without LangChain → you just call OpenAI or HuggingFace directly.
With LangChain →
+ Unified way to use different models
+ Easy chaining (combine steps)
+ Add memory, tools, and agents later

Think of it as: LangChain makes raw models smarter and easier to use in big projects.

## 6. Quick Analogy

Model = Brain, LangChain = Nervous system connecting the brain to memory, tools, and environment and You = The one asking questions! So, the first block in LangChain is Models → the foundation.

---

❤️***Stay Tuned for more!*** 
